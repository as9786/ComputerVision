{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCtGN4sZbeHI9Ghg6YC5jb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/as9786/ComputerVision/blob/main/ImageSegmentation/code/Segformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install"
      ],
      "metadata": {
        "id": "_blYmZLEM4u1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDw9D9tOMt6B",
        "outputId": "215d54b5-fd72-49a4-84be-d6eb1d8d9b4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install einops"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 함수"
      ],
      "metadata": {
        "id": "NLmKR6CuM7Gz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from einops import rearrange\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "vWVkxAD3M4Hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 층 정규화\n",
        "\n",
        "class LayerNormalization2D(nn.LayerNorm):\n",
        "\n",
        "  def forward(self,x):\n",
        "    # 형식 바꾸기\n",
        "    x = rearrange(x, 'b c h w -> b h w c')\n",
        "    # 층 정규화\n",
        "    x = super().forward(x)\n",
        "    # 원래 모양으로 복구\n",
        "    x = rearrange(x, 'b h w c -> b c h w')\n",
        "    return x"
      ],
      "metadata": {
        "id": "Tsl-nh7gNCc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch의 nn.LayerNorm 함수는 (batch, ..., channels)의 입력을 받기 때문에 torch 자체에서는 영상을 (channels, h, w)로 가지기 때문에 이를 바꿔주었다"
      ],
      "metadata": {
        "id": "InGFf1TbNXL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Overlap patch merging\n",
        "\n",
        "class  OverlapPatchMerging(nn.Sequential):\n",
        "\n",
        "  def __init__(self, in_channels, out_channels, patch_size, overlap_size):\n",
        "\n",
        "    super().__init__(nn.Conv2d(in_channels, out_channels, kernel_size = patch_size, stride = overlap_size,\n",
        "                               padding = patch_size // 2, bias = False),\n",
        "                     LayerNormalization2D(out_channels))"
      ],
      "metadata": {
        "id": "AJs9twLnNWyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Efficient multi-head attention\n",
        "\n",
        "class EfficientMultiHeadAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, channels, reduction_ratio = 1, num_heads = 8):\n",
        "    super().__init__()\n",
        "    self.reducer = nn.Sequential(nn.Conv2d(channels, channels, kernel_size = reduction_ratio,stride = reduction_ratio),\n",
        "                                  LayerNormalization2D(channels))\n",
        "\n",
        "    self.att = nn.MultiheadAttention(channels, num_heads = num_heads, batch_first = True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    _, _, h, w = x.shape\n",
        "    reduced_x = self.reducer(x)\n",
        "    reduced_x = rearrange(reduced_x, 'b c h w -> b (h w) c')\n",
        "    x = rearrange(x, 'b c h w -> b (h w) c')\n",
        "    out = self.att(x,reduced_x, reduced_x)[0]\n",
        "\n",
        "    out = rearrange(out, 'b (h w) c -> b c h w',h=h,w=w)\n",
        "    return out"
      ],
      "metadata": {
        "id": "EKHidaIQOL65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn((1,8,64,64))\n",
        "block = EfficientMultiHeadAttention(8,4)\n",
        "block(x).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vX4bN9VWPLpx",
        "outputId": "bf0d7933-c5a4-46c4-d9bf-05dbf509513f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 8, 64, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "논문에서는 선형 변환을 통해서 가중치를 줄였지만 convolution filter를 통해서도 해당 문제를 해결 가능\n",
        "\n",
        "Parameter의 수는 증가하지만 transformer에서의 지역 정보가 부족한 부분을 채워줄 수 있음"
      ],
      "metadata": {
        "id": "730YzxE8PenL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Efficient multi-head attention in paper\n",
        "\n",
        "class EfficientMultiHeadAttentionInPaper(nn.Module):\n",
        "\n",
        "  def __init__(self,channels, reduction_ratio):\n",
        "    super().__init__()\n",
        "    self.channels = channels\n",
        "    self.reduction_ratio = reduction_ratio\n",
        "    self.att = nn.MultiheadAttention(channels,num_heads=8,batch_first=True)\n",
        "    self.reducer = nn.Linear(channels * reduction_ratio, channels)\n",
        "  def forward(self,x):\n",
        "    _, _, h, w = x.shape\n",
        "    x = rearrange(x, 'b c h w -> b (h w) c')\n",
        "    reduced_x = rearrange(x, \"b (hw r) c -> b hw (c r)\", r=4)\n",
        "    reduced_x = self.reducer(reduced_x)\n",
        "\n",
        "    out = self.att(x,reduced_x,reduced_x)[0]\n",
        "    out = rearrange(out,'b (h w) c -> b c h w',h=h)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "KkzuVD91PdRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn((1,8,64,64))\n",
        "block = EfficientMultiHeadAttentionInPaper(8,4)\n",
        "block(x).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fc3qKX32UQwT",
        "outputId": "ac39aba2-a7e1-495a-80f4-83077a0acae8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 8, 64, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsummary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-cJtJYlS5JG",
        "outputId": "bd309d64-a0d2-4a11-d21d-b9a2c41e1ee2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary as summary"
      ],
      "metadata": {
        "id": "rEglHBRMTk51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_method = EfficientMultiHeadAttention(8,4)\n",
        "summary(conv_method,(8,64,64))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4KYaGhXTe-u",
        "outputId": "79531d41-276f-47de-e5a5-3412f48a6d5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 8, 16, 16]           1,032\n",
            "LayerNormalization2D-2            [-1, 8, 16, 16]              16\n",
            "MultiheadAttention-3  [[-1, 4096, 8], [-1, 4096, 256]]               0\n",
            "================================================================\n",
            "Total params: 1,048\n",
            "Trainable params: 1,048\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.12\n",
            "Forward/backward pass size (MB): 262143.97\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 262144.10\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paper_method = EfficientMultiHeadAttentionInPaper(8,4)\n",
        "summary(paper_method,(8,64,64))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGHnnYLwTp8b",
        "outputId": "2463b16a-7b4f-4ef0-a738-010afd45f413"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1              [-1, 1024, 8]             264\n",
            "MultiheadAttention-2  [[-1, 4096, 8], [-1, 4096, 1024]]               0\n",
            "================================================================\n",
            "Total params: 264\n",
            "Trainable params: 264\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.12\n",
            "Forward/backward pass size (MB): 1048575.94\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 1048576.06\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mix-MLP\n",
        "\n",
        "class MixMLP(nn.Sequential):\n",
        "\n",
        "  def __init__(self, channels, expansion=4):\n",
        "    super().__init__(nn.Conv2d(channels, channels, kernel_size=1),\n",
        "                     nn.Conv2d(channels,channels * expansion,kernel_size=3,groups=channels,padding=1),\n",
        "                     nn.GELU(), nn.Conv2d(channels*expansion, channels, kernel_size = 1))"
      ],
      "metadata": {
        "id": "S51oKxkXUrWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.ops import StochasticDepth"
      ],
      "metadata": {
        "id": "qFKDf44WXyQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualAdd(nn.Module):\n",
        "\n",
        "  def __init__(self, fn):\n",
        "    super().__init__()\n",
        "    self.fn = fn\n",
        "\n",
        "  def forward(self, x, **kwargs):\n",
        "    out = self.fn(x, **kwargs)\n",
        "    x = x + out\n",
        "    return x"
      ],
      "metadata": {
        "id": "QdBGPPUqX41n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SegFormerEncoderBlock(nn.Sequential):\n",
        "\n",
        "  def __init__(self, channels, reduction_ratio=1, num_heads = 8, mlp_expansion = 4, drop_path_prob = 0):\n",
        "\n",
        "    super().__init__(ResidualAdd(nn.Sequential(LayerNormalization2D(channels),\n",
        "                                               EfficientMultiHeadAttention(channels,reduction_ratio,num_heads))),\n",
        "                                 ResidualAdd(nn.Sequential(LayerNormalization2D(channels),\n",
        "                                                           MixMLP(channels,expansion=mlp_expansion),\n",
        "                                                           StochasticDepth(p=drop_path_prob,mode='batch'))))"
      ],
      "metadata": {
        "id": "njeF86_JYGB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn((1,8,64,64))\n",
        "block = SegFormerEncoderBlock(8,4)\n",
        "block(x).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-N_uiSnYYpf7",
        "outputId": "05290eac-cde1-4a5b-c7ca-2753157fafdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 8, 64, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Iterable\n",
        "from typing import List"
      ],
      "metadata": {
        "id": "hppJPLTOafh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SegFormerEncoderStage(nn.Sequential):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        patch_size,\n",
        "        overlap_size,\n",
        "        drop_probs,\n",
        "        depth,\n",
        "        reduction_ratio,\n",
        "        num_heads,\n",
        "        mlp_expansion,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.overlap_patch_merge = OverlapPatchMerging(\n",
        "            in_channels, out_channels, patch_size, overlap_size,\n",
        "        )\n",
        "        self.blocks = nn.Sequential(\n",
        "            *[\n",
        "                SegFormerEncoderBlock(\n",
        "                    out_channels, reduction_ratio, num_heads, mlp_expansion, drop_probs[i]\n",
        "                )\n",
        "                for i in range(depth)\n",
        "            ]\n",
        "        )\n",
        "        self.norm = LayerNormalization2D(out_channels)"
      ],
      "metadata": {
        "id": "kBltemfSayRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunks(data, sizes):\n",
        "\n",
        "    curr = 0\n",
        "    for size in sizes:\n",
        "        chunk = data[curr: curr + size]\n",
        "        curr += size\n",
        "        yield chunk"
      ],
      "metadata": {
        "id": "QmNlNrBMahBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SegFormerEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        widths,\n",
        "        depths,\n",
        "        all_num_heads,\n",
        "        patch_sizes,\n",
        "        overlap_sizes,\n",
        "        reduction_ratios,\n",
        "        mlp_expansions,\n",
        "        drop_prob\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        drop_probs =  [x.item() for x in torch.linspace(0, drop_prob, sum(depths))]\n",
        "        self.stages = nn.ModuleList(\n",
        "            [\n",
        "                SegFormerEncoderStage(*args)\n",
        "                for args in zip(\n",
        "                    [in_channels, *widths],\n",
        "                    widths,\n",
        "                    patch_sizes,\n",
        "                    overlap_sizes,\n",
        "                    chunks(drop_probs, sizes=depths),\n",
        "                    depths,\n",
        "                    reduction_ratios,\n",
        "                    all_num_heads,\n",
        "                    mlp_expansions\n",
        "                )\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = []\n",
        "        for stage in self.stages:\n",
        "            x = stage(x)\n",
        "            features.append(x)\n",
        "        return features"
      ],
      "metadata": {
        "id": "X3BLkgMdalgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SegFormerDecoderBlock(nn.Sequential):\n",
        "    def __init__(self, in_channels: int, out_channels: int, scale_factor: int = 2):\n",
        "        super().__init__(\n",
        "            nn.UpsamplingBilinear2d(scale_factor=scale_factor),\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
        "        )"
      ],
      "metadata": {
        "id": "AjzShmiKbBcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SegFormerDecoder(nn.Module):\n",
        "    def __init__(self, out_channels: int, widths: List[int], scale_factors: List[int]):\n",
        "        super().__init__()\n",
        "        self.stages = nn.ModuleList(\n",
        "            [\n",
        "                SegFormerDecoderBlock(in_channels, out_channels, scale_factor)\n",
        "                for in_channels, scale_factor in zip(widths, scale_factors)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def forward(self, features):\n",
        "        new_features = []\n",
        "        for feature, stage in zip(features,self.stages):\n",
        "            x = stage(feature)\n",
        "            new_features.append(x)\n",
        "        return new_features"
      ],
      "metadata": {
        "id": "IysyHtX5bgfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SegFormerSegmentationHead(nn.Module):\n",
        "    def __init__(self, channels: int, num_classes: int, num_features: int = 4):\n",
        "        super().__init__()\n",
        "        self.fuse = nn.Sequential(\n",
        "            nn.Conv2d(channels * num_features, channels, kernel_size=1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(channels)\n",
        "        )\n",
        "        self.predict = nn.Conv2d(channels, num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, features):\n",
        "        x = torch.cat(features, dim=1)\n",
        "        x = self.fuse(x)\n",
        "        x = self.predict(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "-6YKmAh6b45c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SegFormer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        widths: List[int],\n",
        "        depths: List[int],\n",
        "        all_num_heads: List[int],\n",
        "        patch_sizes: List[int],\n",
        "        overlap_sizes: List[int],\n",
        "        reduction_ratios: List[int],\n",
        "        mlp_expansions: List[int],\n",
        "        decoder_channels: int,\n",
        "        scale_factors: List[int],\n",
        "        num_classes: int,\n",
        "        drop_prob: float = 0.0,\n",
        "    ):\n",
        "\n",
        "        super().__init__()\n",
        "        self.encoder = SegFormerEncoder(\n",
        "            in_channels,\n",
        "            widths,\n",
        "            depths,\n",
        "            all_num_heads,\n",
        "            patch_sizes,\n",
        "            overlap_sizes,\n",
        "            reduction_ratios,\n",
        "            mlp_expansions,\n",
        "            drop_prob,\n",
        "        )\n",
        "        self.decoder = SegFormerDecoder(decoder_channels, widths[::-1], scale_factors)\n",
        "        self.head = SegFormerSegmentationHead(\n",
        "            decoder_channels, num_classes, num_features=len(widths)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.encoder(x)\n",
        "        features = self.decoder(features[::-1])\n",
        "        segmentation = self.head(features)\n",
        "        return segmentation"
      ],
      "metadata": {
        "id": "q4nt0J_TcAcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "segformer = SegFormer(\n",
        "    in_channels=3,\n",
        "    widths=[64, 128, 256, 512],\n",
        "    depths=[3, 4, 6, 3],\n",
        "    all_num_heads=[1, 2, 4, 8],\n",
        "    patch_sizes=[7, 3, 3, 3],\n",
        "    overlap_sizes=[4, 2, 2, 2],\n",
        "    reduction_ratios=[8, 4, 2, 1],\n",
        "    mlp_expansions=[4, 4, 4, 4],\n",
        "    decoder_channels=256,\n",
        "    scale_factors=[8, 4, 2, 1],\n",
        "    num_classes=100,\n",
        ")"
      ],
      "metadata": {
        "id": "Mn7DZfb5cB0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segmentation = segformer(torch.randn((4, 3, 224, 224)))"
      ],
      "metadata": {
        "id": "q0wB4yjCcC-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segmentation.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPcEKaWxcD79",
        "outputId": "ac314628-b6d9-4193-bff5-7049684e177e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 100, 56, 56])"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6aIqmvvNcFA5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}