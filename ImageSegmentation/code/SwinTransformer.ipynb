{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/as9786/ComputerVision/blob/main/SwinTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esUQufxsZyCX",
        "outputId": "37bb0fc9-58f1-49e6-c50e-13d4b0a5f0f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'swin-transformer-pytorch'...\n",
            "remote: Enumerating objects: 80, done.\u001b[K\n",
            "remote: Counting objects: 100% (80/80), done.\u001b[K\n",
            "remote: Compressing objects: 100% (55/55), done.\u001b[K\n",
            "remote: Total 80 (delta 39), reused 62 (delta 21), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (80/80), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/berniwal/swin-transformer-pytorch.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_016csqZZ6q6"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8ml7rBkHaGBr",
        "outputId": "101858d0-af89-43d8-ea0b-5c7652cdeb49"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZYe7SaUaPQF"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/swin-transformer-pytorch')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "rYA3uzF2acEX",
        "outputId": "11cdc653-ab89-4231-edbe-e3a2625f3be0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.8.1\n",
            "  Downloading torch-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (804.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 804.1 MB 2.8 kB/s \n",
            "\u001b[?25hCollecting einops==0.3.0\n",
            "  Downloading einops-0.3.0-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->-r requirements.txt (line 1)) (4.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->-r requirements.txt (line 1)) (1.21.6)\n",
            "Installing collected packages: torch, einops\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.8.1 which is incompatible.\n",
            "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.8.1 which is incompatible.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.8.1 which is incompatible.\u001b[0m\n",
            "Successfully installed einops-0.3.0 torch-1.8.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AB1EkeInkB9P",
        "outputId": "536c977b-52b4-4ca7-a830-1db938e0f744"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 1.0297e+00,  7.2482e-01, -5.7784e-01,  ...,  2.1716e+00,\n",
              "           -1.1715e-01, -1.1382e+00],\n",
              "          [-8.6904e-02, -1.1003e+00,  3.3664e-01,  ...,  9.5130e-01,\n",
              "           -9.3621e-01,  5.5656e-01],\n",
              "          [ 1.0366e+00, -1.6543e+00,  7.0330e-01,  ..., -2.9983e-01,\n",
              "           -1.0263e+00, -1.4405e+00],\n",
              "          ...,\n",
              "          [ 1.3878e-01,  7.5073e-01, -3.8704e-01,  ...,  7.4716e-01,\n",
              "            7.3229e-01,  2.1427e-01],\n",
              "          [-8.8091e-01, -1.0219e+00, -9.1448e-01,  ..., -1.1691e-01,\n",
              "            1.4744e+00, -4.5648e-02],\n",
              "          [ 1.7411e-01,  8.8788e-01,  4.0662e-01,  ...,  1.9527e+00,\n",
              "           -7.9396e-01,  1.5423e-01]],\n",
              "\n",
              "         [[-2.3063e-02, -1.7782e+00, -8.3196e-01,  ...,  1.9940e+00,\n",
              "            6.8300e-01,  1.1638e+00],\n",
              "          [-1.6248e+00, -1.0328e+00,  1.1868e+00,  ..., -2.5906e+00,\n",
              "            5.0713e-01, -6.8978e-01],\n",
              "          [ 1.9960e-01, -1.9141e+00,  3.9732e-01,  ..., -7.8667e-01,\n",
              "            3.8837e-01,  9.5256e-01],\n",
              "          ...,\n",
              "          [ 1.0989e-01,  1.3162e-01,  6.9272e-01,  ...,  2.7898e-01,\n",
              "            9.5510e-01,  1.0027e+00],\n",
              "          [-7.2110e-01,  1.4588e-01, -5.9900e-02,  ..., -3.6249e-01,\n",
              "           -1.2383e+00,  6.4151e-01],\n",
              "          [-3.7122e-01, -1.4255e+00,  9.8310e-01,  ..., -1.4180e+00,\n",
              "            1.5400e+00, -8.1682e-01]],\n",
              "\n",
              "         [[ 2.2034e+00,  3.2149e+00,  2.7712e-01,  ..., -1.0116e-02,\n",
              "           -6.3982e-01, -1.3076e-01],\n",
              "          [-1.3237e+00, -1.1877e+00,  2.2784e-01,  ..., -1.8421e-01,\n",
              "           -6.1347e-01,  8.6368e-01],\n",
              "          [ 1.0377e+00, -1.1421e-01,  1.0149e+00,  ...,  9.0517e-01,\n",
              "           -1.1434e+00, -5.5301e-01],\n",
              "          ...,\n",
              "          [ 7.8790e-01,  3.4331e-01, -1.6745e+00,  ...,  6.7523e-01,\n",
              "            5.4290e-01, -1.4374e+00],\n",
              "          [-1.4759e-01, -1.1233e+00,  1.6267e+00,  ...,  5.0939e-01,\n",
              "            8.8296e-01, -1.0751e-01],\n",
              "          [-7.2400e-01,  1.9601e+00, -7.5096e-01,  ..., -5.7919e-02,\n",
              "           -3.9404e-01, -2.9906e-01]]],\n",
              "\n",
              "\n",
              "        [[[-1.6683e+00, -1.2475e+00,  1.1863e+00,  ..., -3.4462e-03,\n",
              "            4.0051e-01, -8.9466e-01],\n",
              "          [ 1.8618e+00, -2.1440e+00, -2.5810e-01,  ..., -2.8776e-01,\n",
              "            8.5106e-01,  8.4357e-02],\n",
              "          [-1.0150e+00, -7.6551e-01, -9.2894e-01,  ..., -9.0590e-02,\n",
              "           -1.3378e+00,  5.0209e-01],\n",
              "          ...,\n",
              "          [-7.0190e-01,  8.8225e-01, -1.6766e+00,  ..., -4.9137e-01,\n",
              "           -2.0508e-02,  7.5392e-01],\n",
              "          [-9.8610e-01,  1.4371e+00,  1.3856e+00,  ..., -1.7936e+00,\n",
              "            1.6592e+00,  2.4680e-01],\n",
              "          [-2.7846e-01,  1.6156e-02, -7.3169e-01,  ..., -2.2178e+00,\n",
              "           -3.6414e-01, -1.9630e+00]],\n",
              "\n",
              "         [[-1.1194e+00, -6.2100e-01, -4.5006e-02,  ...,  6.1516e-01,\n",
              "           -4.5998e-01, -6.3078e-01],\n",
              "          [ 4.2743e-02, -4.2419e-01, -6.2715e-01,  ...,  1.9091e+00,\n",
              "            9.5674e-01,  6.9594e-01],\n",
              "          [ 1.1015e+00, -1.6794e+00, -9.1084e-01,  ..., -1.2324e-01,\n",
              "            1.7176e+00,  8.9833e-02],\n",
              "          ...,\n",
              "          [ 9.3568e-01, -6.4169e-01,  2.3155e+00,  ..., -1.7776e+00,\n",
              "            1.4079e-01, -2.2644e-01],\n",
              "          [-1.4416e+00,  1.2467e+00, -5.0037e-01,  ..., -1.4273e-01,\n",
              "           -1.1004e-01, -2.0350e-01],\n",
              "          [-7.7708e-01,  3.4346e-01,  3.6481e-02,  ..., -6.3473e-03,\n",
              "            8.5014e-01, -1.1760e+00]],\n",
              "\n",
              "         [[-9.9252e-01,  1.8844e+00,  9.0589e-01,  ..., -9.8262e-01,\n",
              "            2.9393e-01, -5.0474e-01],\n",
              "          [-1.4741e+00,  1.1404e-01, -7.3199e-01,  ...,  2.1057e-01,\n",
              "           -1.2704e+00, -5.4253e-03],\n",
              "          [-2.1192e+00, -1.4119e+00,  1.8784e+00,  ..., -8.2131e-02,\n",
              "            2.6247e-01,  8.1594e-01],\n",
              "          ...,\n",
              "          [ 1.9048e+00,  5.3464e-01, -1.1420e+00,  ..., -8.9387e-01,\n",
              "            1.2630e-01,  7.1493e-01],\n",
              "          [ 2.1069e+00, -5.3030e-01,  6.7028e-01,  ..., -6.6742e-01,\n",
              "            7.8061e-01, -1.4275e-01],\n",
              "          [-3.7926e-01, -1.3673e+00, -1.7520e+00,  ...,  1.7790e+00,\n",
              "           -7.8631e-01, -6.7287e-01]]],\n",
              "\n",
              "\n",
              "        [[[-4.9497e-01,  6.4774e-01,  8.3765e-01,  ...,  1.0609e+00,\n",
              "           -1.2846e+00,  1.5826e-01],\n",
              "          [ 9.8281e-01, -3.6340e-01,  1.8931e+00,  ...,  2.6592e-01,\n",
              "            7.9294e-01, -3.1165e-01],\n",
              "          [-2.0463e+00,  1.0747e+00, -2.1899e-01,  ...,  2.0337e+00,\n",
              "           -1.7326e+00, -1.3682e+00],\n",
              "          ...,\n",
              "          [-7.2166e-01, -9.6093e-01, -6.3381e-01,  ..., -1.4507e-04,\n",
              "            3.6101e-01, -5.3341e-01],\n",
              "          [ 1.6125e+00,  1.4274e+00, -2.7368e-01,  ..., -4.8806e-01,\n",
              "            3.5951e-01,  3.1694e-01],\n",
              "          [ 5.9234e-01,  4.5095e-01,  1.3613e-01,  ..., -6.6308e-01,\n",
              "            6.1493e-01, -5.2865e-01]],\n",
              "\n",
              "         [[ 4.7571e-01, -3.0205e-01, -1.6785e+00,  ...,  2.6457e-02,\n",
              "            7.1032e-01,  2.9897e-01],\n",
              "          [-9.6744e-01, -1.5922e+00,  1.9925e+00,  ..., -2.9482e-01,\n",
              "            1.1398e-01, -1.4087e+00],\n",
              "          [-4.6242e-01, -1.6587e-01, -5.1557e-01,  ...,  4.9644e-01,\n",
              "           -7.8176e-01, -1.1696e+00],\n",
              "          ...,\n",
              "          [ 2.4175e-01,  7.4891e-02,  4.5345e-01,  ...,  4.3853e-02,\n",
              "           -4.5835e-01,  8.1764e-01],\n",
              "          [ 3.8077e-01, -4.5072e-01, -2.6217e-02,  ...,  1.0047e+00,\n",
              "           -3.1405e+00, -6.4333e-01],\n",
              "          [ 5.6027e-01,  3.8660e-01,  2.2580e+00,  ..., -6.6334e-01,\n",
              "           -1.3101e+00, -1.3515e+00]],\n",
              "\n",
              "         [[-1.5481e-01,  1.0950e-01, -1.3846e+00,  ...,  8.5784e-01,\n",
              "            1.0386e+00, -2.4685e-01],\n",
              "          [ 1.9002e-01, -3.7176e-01,  9.2857e-01,  ..., -1.8056e-01,\n",
              "           -4.0632e-01, -2.7868e-01],\n",
              "          [-9.1443e-01,  1.0301e+00,  1.2299e+00,  ..., -1.4740e+00,\n",
              "           -3.6001e-02, -2.2090e+00],\n",
              "          ...,\n",
              "          [-7.4493e-01, -7.8522e-01,  2.2154e-01,  ...,  5.1924e-01,\n",
              "           -1.1942e+00, -1.0385e+00],\n",
              "          [ 7.4819e-01,  8.1657e-01,  3.5489e+00,  ...,  1.8182e+00,\n",
              "           -3.2917e-02,  1.5100e+00],\n",
              "          [ 1.4903e-01, -1.2404e+00,  4.2135e-01,  ...,  7.9328e-02,\n",
              "            1.3630e-01,  1.7484e+00]]],\n",
              "\n",
              "\n",
              "        [[[ 3.6092e-02, -6.8632e-01,  1.1659e+00,  ...,  1.3852e-01,\n",
              "            2.6261e-01,  8.4736e-01],\n",
              "          [-1.4257e+00, -1.3933e+00,  1.2137e-02,  ...,  1.4392e+00,\n",
              "           -1.0728e-02, -9.6954e-01],\n",
              "          [-2.3307e+00,  6.9455e-01,  9.1790e-01,  ..., -3.8177e-01,\n",
              "            2.3724e+00,  5.9591e-01],\n",
              "          ...,\n",
              "          [-6.1067e-01, -5.4670e-01, -1.4283e+00,  ..., -3.3911e-01,\n",
              "            2.3023e-01,  4.4205e-01],\n",
              "          [ 7.1264e-01,  1.2379e+00, -2.2255e+00,  ..., -1.7645e+00,\n",
              "            1.5061e+00, -1.6148e-01],\n",
              "          [ 2.0779e+00, -5.5093e-01, -5.0541e-01,  ...,  8.0193e-01,\n",
              "            5.7588e-01,  7.4345e-01]],\n",
              "\n",
              "         [[-1.2548e+00,  8.0272e-01,  1.4307e+00,  ..., -1.6567e+00,\n",
              "           -1.4381e-01, -1.5653e+00],\n",
              "          [ 4.8103e-01, -1.3358e-01, -6.7548e-01,  ..., -9.0745e-01,\n",
              "           -3.0939e-01,  4.3025e-01],\n",
              "          [ 5.7914e-01, -3.7210e-01, -9.1391e-01,  ...,  9.0010e-01,\n",
              "            1.0974e+00, -2.3876e-01],\n",
              "          ...,\n",
              "          [ 1.1512e+00,  7.2011e-01, -7.7372e-01,  ..., -1.2368e-01,\n",
              "            3.9068e-01, -1.6424e+00],\n",
              "          [-9.7730e-02, -1.4165e+00, -6.2112e-02,  ..., -7.4610e-01,\n",
              "            6.1177e-01, -7.7219e-01],\n",
              "          [-7.8328e-01, -1.3486e+00,  5.2982e-01,  ...,  6.2892e-01,\n",
              "           -4.1221e-01, -1.6767e-01]],\n",
              "\n",
              "         [[-2.8497e-02, -7.6065e-01,  2.6891e-01,  ..., -3.8069e-01,\n",
              "           -5.3537e-01,  1.9123e+00],\n",
              "          [-1.6619e-01, -1.6120e-01,  1.2387e+00,  ..., -1.1953e+00,\n",
              "           -6.8358e-01, -1.0411e+00],\n",
              "          [-1.5837e+00, -5.1571e-01,  6.7774e-01,  ...,  7.1443e-01,\n",
              "            1.3566e+00,  4.0756e-01],\n",
              "          ...,\n",
              "          [-1.4725e+00,  8.1311e-01, -3.4974e-01,  ...,  1.2772e-01,\n",
              "            6.3577e-01,  1.0826e+00],\n",
              "          [ 1.7709e-01,  5.6522e-01,  9.7913e-01,  ...,  7.5357e-01,\n",
              "           -1.0411e-01,  3.4425e-01],\n",
              "          [ 1.5593e+00, -1.0993e-01,  1.3569e+00,  ..., -1.8922e+00,\n",
              "           -2.4073e+00,  1.4558e+00]]],\n",
              "\n",
              "\n",
              "        [[[ 3.2189e-01,  4.6613e-01,  5.5415e-01,  ...,  5.5103e-01,\n",
              "           -5.8145e-01,  6.4699e-01],\n",
              "          [-8.1125e-01,  1.3743e-02,  6.5210e-01,  ...,  2.9025e+00,\n",
              "            1.8862e-01,  1.1929e+00],\n",
              "          [ 7.0661e-01, -4.7266e-01, -9.6115e-01,  ...,  1.1737e+00,\n",
              "           -1.9048e+00,  1.0663e+00],\n",
              "          ...,\n",
              "          [-5.2618e-01,  1.5544e-01,  7.2481e-01,  ..., -2.4019e-02,\n",
              "           -2.6096e-02, -4.9541e-01],\n",
              "          [-1.1620e+00, -4.5347e-01, -8.5165e-01,  ...,  1.4698e+00,\n",
              "           -8.1202e-01,  7.2389e-01],\n",
              "          [-1.5688e+00,  5.8332e-01,  8.3200e-01,  ...,  2.4731e-01,\n",
              "           -7.3746e-01,  6.1301e-01]],\n",
              "\n",
              "         [[-2.1833e+00, -2.3283e+00,  5.3271e-01,  ...,  1.7300e-01,\n",
              "           -8.9355e-02,  1.3164e+00],\n",
              "          [-7.6578e-01, -6.4835e-02, -7.1066e-01,  ...,  5.1294e-01,\n",
              "           -1.1482e+00,  7.2350e-01],\n",
              "          [-1.4626e+00,  8.4435e-01, -1.0098e+00,  ...,  1.9782e+00,\n",
              "            1.7957e-01, -6.7619e-01],\n",
              "          ...,\n",
              "          [ 7.6088e-01, -4.2478e-01,  8.0743e-01,  ...,  1.3120e+00,\n",
              "            4.9795e-01,  1.8476e+00],\n",
              "          [-1.3234e+00,  9.5360e-01,  2.7421e-02,  ..., -1.0893e-01,\n",
              "           -2.1908e-03,  8.4169e-01],\n",
              "          [ 7.9471e-01, -8.0446e-02, -1.7834e+00,  ..., -6.6339e-01,\n",
              "           -6.0689e-01, -6.5971e-01]],\n",
              "\n",
              "         [[ 9.0205e-01, -1.4976e+00, -1.6475e+00,  ..., -3.1594e-01,\n",
              "           -3.5663e-02,  2.4583e+00],\n",
              "          [ 1.1088e-01, -1.4632e+00, -1.7392e+00,  ...,  2.1795e+00,\n",
              "            9.5548e-01, -1.6594e-02],\n",
              "          [-1.1155e+00, -1.1816e+00,  9.2978e-01,  ..., -9.5906e-01,\n",
              "            6.0415e-02, -9.9743e-03],\n",
              "          ...,\n",
              "          [-5.1458e-02, -7.1443e-01,  2.1012e+00,  ...,  1.3720e+00,\n",
              "            1.3580e+00, -3.9853e-01],\n",
              "          [ 9.2125e-01, -4.5298e-01, -4.9973e-01,  ...,  1.5357e+00,\n",
              "           -2.0437e+00,  3.0887e-01],\n",
              "          [ 1.4853e+00, -7.1520e-01, -4.1371e-01,  ...,  1.8005e+00,\n",
              "            7.8316e-01,  3.9405e-03]]],\n",
              "\n",
              "\n",
              "        [[[-5.9806e-01,  3.9176e-01, -1.5988e-02,  ..., -8.7292e-01,\n",
              "           -4.4002e-01,  9.6778e-01],\n",
              "          [-2.1237e-01,  4.8269e-01, -9.3776e-01,  ...,  1.1718e+00,\n",
              "            4.8898e-01,  1.0324e+00],\n",
              "          [-1.6955e+00,  1.2351e+00,  2.5033e+00,  ..., -1.0531e+00,\n",
              "            5.6811e-01,  2.0825e+00],\n",
              "          ...,\n",
              "          [ 1.4355e+00,  1.0343e+00, -1.1266e+00,  ..., -2.3449e+00,\n",
              "           -1.7896e+00, -3.0548e-01],\n",
              "          [ 4.5272e-01,  1.7430e+00,  1.9224e+00,  ..., -1.3701e+00,\n",
              "           -1.4485e+00, -2.6393e-01],\n",
              "          [-1.6070e-01,  1.7590e-01,  8.0434e-01,  ..., -1.3974e+00,\n",
              "            1.0189e+00, -2.3666e+00]],\n",
              "\n",
              "         [[ 8.1550e-01,  4.0902e-01, -3.7687e-01,  ..., -4.3359e-01,\n",
              "            3.4508e-01,  9.1665e-02],\n",
              "          [-2.4533e-01, -1.2751e+00,  1.5354e+00,  ..., -3.2606e-01,\n",
              "            1.6894e+00,  4.1282e-01],\n",
              "          [-2.2286e-01,  1.2414e+00,  1.9881e+00,  ...,  2.2354e-01,\n",
              "           -3.7801e-01, -5.4567e-01],\n",
              "          ...,\n",
              "          [-7.5298e-01,  6.2687e-01, -1.2453e+00,  ..., -6.6674e-01,\n",
              "            9.2765e-01,  7.5240e-02],\n",
              "          [ 3.6888e-01, -3.9333e-01, -4.4741e-02,  ...,  9.1083e-02,\n",
              "           -3.6179e-01, -2.6045e-01],\n",
              "          [-5.9312e-01,  2.6481e+00,  1.2147e-01,  ...,  7.9591e-01,\n",
              "           -2.4252e-01, -8.2617e-01]],\n",
              "\n",
              "         [[ 4.2721e-01, -1.1900e+00,  2.3503e+00,  ...,  1.4769e+00,\n",
              "            2.2780e-01,  1.4578e+00],\n",
              "          [-8.0546e-01, -8.1797e-01,  1.6344e+00,  ...,  1.2846e+00,\n",
              "           -5.9077e-01, -9.1087e-01],\n",
              "          [ 1.1395e+00,  1.2571e+00, -1.7973e+00,  ..., -1.2064e+00,\n",
              "            8.5449e-01,  3.4717e-01],\n",
              "          ...,\n",
              "          [-5.6332e-01,  1.2952e-01, -7.4212e-01,  ..., -7.3292e-02,\n",
              "           -1.1999e+00,  1.3054e+00],\n",
              "          [ 5.1147e-01,  1.3962e+00,  3.3137e-02,  ...,  1.0016e+00,\n",
              "           -1.3968e+00, -4.0474e-01],\n",
              "          [ 2.3588e-01, -1.7647e+00, -1.1022e+00,  ...,  5.6510e-01,\n",
              "            9.5538e-01,  4.2936e-01]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "pShrtoBFaHkX",
        "outputId": "35079bcf-ade8-4cd6-ecab-d82551215730"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-56c6dd954219>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mswin_transformer_pytorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSwinTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m net = SwinTransformer(\n\u001b[1;32m      5\u001b[0m     \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m96\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/swin-transformer-pytorch/swin_transformer_pytorch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mswin_transformer_pytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswin_transformer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSwinTransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswin_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswin_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswin_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswin_l\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/swin-transformer-pytorch/swin_transformer_pytorch/swin_transformer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meinsum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0meinops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrearrange\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'einops'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import torch\n",
        "from swin_transformer_pytorch import SwinTransformer\n",
        "\n",
        "net = SwinTransformer(\n",
        "    hidden_dim=96,\n",
        "    layers=(2, 2, 6, 2),\n",
        "    heads=(3, 6, 12, 24),\n",
        "    channels=3,\n",
        "    num_classes=10,\n",
        "    head_dim=32,\n",
        "    window_size=7,\n",
        "    downscaling_factors=(4, 2, 2, 2),\n",
        "    relative_pos_embedding=True\n",
        ")\n",
        "dummy_x = torch.randn(6, 3, 224, 224)\n",
        "logits = net(dummy_x)  # (1,3)\n",
        "print(net)\n",
        "print(logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhbM8sBVaLfH",
        "outputId": "0e169e89-400a-4696-94ed-6bfa4ee9c040"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.2140,  0.1508, -0.5956],\n",
              "        [-0.0418,  0.1765, -0.6170],\n",
              "        [-0.0033,  0.2613, -0.5147],\n",
              "        [ 0.0816, -0.0297, -0.6763],\n",
              "        [-0.2225,  0.1562, -0.5580],\n",
              "        [-0.0707,  0.0305, -0.6914]], grad_fn=<AddmmBackward>)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "G58efqSreAig",
        "outputId": "a12e1c65-b745-4b9c-e0ba-d9a06e18b92e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.1+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchvision) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Collecting torch==1.12.1\n",
            "  Downloading torch-1.12.1-cp37-cp37m-manylinux1_x86_64.whl (776.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.3 MB 10 kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.8.1\n",
            "    Uninstalling torch-1.8.1:\n",
            "      Successfully uninstalled torch-1.8.1\n",
            "Successfully installed torch-1.12.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FTqj7Eucq7f"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision import transforms\n",
        "trans = transforms.Compose([transforms.Resize((224,224)), \n",
        "                            transforms.ToTensor(),\n",
        "                            transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])"
      ],
      "metadata": {
        "id": "2KUAdg_skhs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLVmPXded-x-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "bd5b0a3e5d82409eb2c7686c8f36f04f",
            "a6ceab93c0b146339ea81cffa959169d",
            "a8a78456c5db49b5b696113b8af721b1",
            "cc217d2367af42ff811dc7a5afa35080",
            "5658b3a0bb2849de99f59840f92fcda2",
            "f189964d47714bfb9f0f7b1c0e253b04",
            "0a5ae50dcf63492a9a447cac3cbe479c",
            "dced840ebad74ceea562c848b8f25fff",
            "2d2b979314d4470592684f290d132e28",
            "af3dd2a7719d4f45bfd6804e35da8c60",
            "8640ff7775134332ae431018b37a8ba7"
          ]
        },
        "outputId": "d65b393e-6cc3-4e1b-e1df-419a7a4621df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd5b0a3e5d82409eb2c7686c8f36f04f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "## MNIST Data down 받기\n",
        "\n",
        "# 공개 데이터셋에서 학습 데이터를 내려받습니다.\n",
        "training_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=trans,\n",
        ")\n",
        "\n",
        "# 공개 데이터셋에서 테스트 데이터를 내려받습니다.\n",
        "test_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=trans,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x,y in training_data:\n",
        "    print(x.shape)\n",
        "    print(y)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCHYx6j_FO3R",
        "outputId": "35d11590-f626-4c65-8930-d998951e71ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 224, 224])\n",
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "# 데이터로더를 생성합니다.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data,batch_size=batch_size)  \n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
        "    print(\"Shape of y: \", y.shape, y.dtype)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxXQPTqBgFU1",
        "outputId": "d5459bec-56f0-488c-e576-65efe864e4e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, C, H, W]:  torch.Size([64, 3, 224, 224])\n",
            "Shape of y:  torch.Size([64]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVprnk66gO4V",
        "outputId": "ba9b4195-954d-44de-be04-8ba5ee8575c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, einsum\n",
        "import numpy as np\n",
        "from einops import rearrange, repeat"
      ],
      "metadata": {
        "id": "60_F4RXoicRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CyclicShift(nn.Module):\n",
        "    def __init__(self, displacement):\n",
        "        super().__init__()\n",
        "        self.displacement = displacement\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.roll(x, shifts=(self.displacement, self.displacement), dims=(1, 2))"
      ],
      "metadata": {
        "id": "Y989q94ugPcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(x, **kwargs) + x"
      ],
      "metadata": {
        "id": "yvu04Ysiiams"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)"
      ],
      "metadata": {
        "id": "3QWuD5zCid-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "D-PATWkXiezl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mask(window_size, displacement, upper_lower, left_right):\n",
        "    mask = torch.zeros(window_size ** 2, window_size ** 2)\n",
        "\n",
        "    if upper_lower:\n",
        "        mask[-displacement * window_size:, :-displacement * window_size] = float('-inf')\n",
        "        mask[:-displacement * window_size, -displacement * window_size:] = float('-inf')\n",
        "\n",
        "    if left_right:\n",
        "        mask = rearrange(mask, '(h1 w1) (h2 w2) -> h1 w1 h2 w2', h1=window_size, h2=window_size)\n",
        "        mask[:, -displacement:, :, :-displacement] = float('-inf')\n",
        "        mask[:, :-displacement, :, -displacement:] = float('-inf')\n",
        "        mask = rearrange(mask, 'h1 w1 h2 w2 -> (h1 w1) (h2 w2)')\n",
        "\n",
        "    return mask"
      ],
      "metadata": {
        "id": "eDNmuYYiifq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_relative_distances(window_size):\n",
        "    indices = torch.tensor(np.array([[x, y] for x in range(window_size) for y in range(window_size)]))\n",
        "    distances = indices[None, :, :] - indices[:, None, :]\n",
        "    return distances"
      ],
      "metadata": {
        "id": "0cVNsN3iig0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WindowAttention(nn.Module):\n",
        "    def __init__(self, dim, heads, head_dim, shifted, window_size, relative_pos_embedding):\n",
        "        super().__init__()\n",
        "        inner_dim = head_dim * heads\n",
        "\n",
        "        self.heads = heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "        self.window_size = window_size\n",
        "        self.relative_pos_embedding = relative_pos_embedding\n",
        "        self.shifted = shifted\n",
        "\n",
        "        if self.shifted:\n",
        "            displacement = window_size // 2\n",
        "            self.cyclic_shift = CyclicShift(-displacement)\n",
        "            self.cyclic_back_shift = CyclicShift(displacement)\n",
        "            self.upper_lower_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement,\n",
        "                                                             upper_lower=True, left_right=False), requires_grad=False)\n",
        "            self.left_right_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement,\n",
        "                                                            upper_lower=False, left_right=True), requires_grad=False)\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
        "\n",
        "        if self.relative_pos_embedding:\n",
        "            self.relative_indices = get_relative_distances(window_size) + window_size - 1\n",
        "            self.pos_embedding = nn.Parameter(torch.randn(2 * window_size - 1, 2 * window_size - 1))\n",
        "        else:\n",
        "            self.pos_embedding = nn.Parameter(torch.randn(window_size ** 2, window_size ** 2))\n",
        "\n",
        "        self.to_out = nn.Linear(inner_dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.shifted:\n",
        "            x = self.cyclic_shift(x)\n",
        "\n",
        "        b, n_h, n_w, _, h = *x.shape, self.heads\n",
        "\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        nw_h = n_h // self.window_size\n",
        "        nw_w = n_w // self.window_size\n",
        "\n",
        "        q, k, v = map(\n",
        "            lambda t: rearrange(t, 'b (nw_h w_h) (nw_w w_w) (h d) -> b h (nw_h nw_w) (w_h w_w) d',\n",
        "                                h=h, w_h=self.window_size, w_w=self.window_size), qkv)\n",
        "\n",
        "        dots = einsum('b h w i d, b h w j d -> b h w i j', q, k) * self.scale\n",
        "\n",
        "        if self.relative_pos_embedding:\n",
        "            dots += self.pos_embedding[self.relative_indices[:, :, 0], self.relative_indices[:, :, 1]]\n",
        "        else:\n",
        "            dots += self.pos_embedding\n",
        "\n",
        "        if self.shifted:\n",
        "            dots[:, :, -nw_w:] += self.upper_lower_mask\n",
        "            dots[:, :, nw_w - 1::nw_w] += self.left_right_mask\n",
        "\n",
        "        attn = dots.softmax(dim=-1)\n",
        "\n",
        "        out = einsum('b h w i j, b h w j d -> b h w i d', attn, v)\n",
        "        out = rearrange(out, 'b h (nw_h nw_w) (w_h w_w) d -> b (nw_h w_h) (nw_w w_w) (h d)',\n",
        "                        h=h, w_h=self.window_size, w_w=self.window_size, nw_h=nw_h, nw_w=nw_w)\n",
        "        out = self.to_out(out)\n",
        "\n",
        "        if self.shifted:\n",
        "            out = self.cyclic_back_shift(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "jaU-bElBihqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SwinBlock(nn.Module):\n",
        "    def __init__(self, dim, heads, head_dim, mlp_dim, shifted, window_size, relative_pos_embedding):\n",
        "        super().__init__()\n",
        "        self.attention_block = Residual(PreNorm(dim, WindowAttention(dim=dim,\n",
        "                                                                     heads=heads,\n",
        "                                                                     head_dim=head_dim,\n",
        "                                                                     shifted=shifted,\n",
        "                                                                     window_size=window_size,\n",
        "                                                                     relative_pos_embedding=relative_pos_embedding)))\n",
        "        self.mlp_block = Residual(PreNorm(dim, FeedForward(dim=dim, hidden_dim=mlp_dim)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.attention_block(x)\n",
        "        x = self.mlp_block(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "NYYHip32ijLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchMerging(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, downscaling_factor):\n",
        "        super().__init__()\n",
        "        self.downscaling_factor = downscaling_factor\n",
        "        self.patch_merge = nn.Unfold(kernel_size=downscaling_factor, stride=downscaling_factor, padding=0)\n",
        "        self.linear = nn.Linear(in_channels * downscaling_factor ** 2, out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        new_h, new_w = h // self.downscaling_factor, w // self.downscaling_factor\n",
        "        x = self.patch_merge(x).view(b, -1, new_h, new_w).permute(0, 2, 3, 1)\n",
        "        x = self.linear(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "ZwUNatvcitLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StageModule(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_dimension, layers, downscaling_factor, num_heads, head_dim, window_size,\n",
        "                 relative_pos_embedding):\n",
        "        super().__init__()\n",
        "        assert layers % 2 == 0, 'Stage layers need to be divisible by 2 for regular and shifted block.'\n",
        "\n",
        "        self.patch_partition = PatchMerging(in_channels=in_channels, out_channels=hidden_dimension,\n",
        "                                            downscaling_factor=downscaling_factor)\n",
        "\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(layers // 2):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
        "                          shifted=False, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n",
        "                SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
        "                          shifted=True, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_partition(x)\n",
        "        for regular_block, shifted_block in self.layers:\n",
        "            x = regular_block(x)\n",
        "            x = shifted_block(x)\n",
        "        return x.permute(0, 3, 1, 2)"
      ],
      "metadata": {
        "id": "_YJIi7nQiudk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SwinTransformer(nn.Module):\n",
        "    def __init__(self, *, hidden_dim, layers, heads, channels=3, num_classes=1000, head_dim=32, window_size=7,\n",
        "                 downscaling_factors=(4, 2, 2, 2), relative_pos_embedding=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.stage1 = StageModule(in_channels=channels, hidden_dimension=hidden_dim, layers=layers[0],\n",
        "                                  downscaling_factor=downscaling_factors[0], num_heads=heads[0], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "        self.stage2 = StageModule(in_channels=hidden_dim, hidden_dimension=hidden_dim * 2, layers=layers[1],\n",
        "                                  downscaling_factor=downscaling_factors[1], num_heads=heads[1], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "        self.stage3 = StageModule(in_channels=hidden_dim * 2, hidden_dimension=hidden_dim * 4, layers=layers[2],\n",
        "                                  downscaling_factor=downscaling_factors[2], num_heads=heads[2], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "        self.stage4 = StageModule(in_channels=hidden_dim * 4, hidden_dimension=hidden_dim * 8, layers=layers[3],\n",
        "                                  downscaling_factor=downscaling_factors[3], num_heads=heads[3], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim * 8),\n",
        "            nn.Linear(hidden_dim * 8, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.stage1(img)\n",
        "        x = self.stage2(x)\n",
        "        x = self.stage3(x)\n",
        "        x = self.stage4(x)\n",
        "        x = x.mean(dim=[2, 3])\n",
        "        return self.mlp_head(x)"
      ],
      "metadata": {
        "id": "KrANCiBkivrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def swin_t(hidden_dim=96, layers=(2, 2, 6, 2), heads=(3, 6, 12, 24), **kwargs):\n",
        "    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)"
      ],
      "metadata": {
        "id": "iSdxjt9-ixZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from swin_transformer_pytorch import swint_t\n",
        "model =  swin_t().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjrx-W-YiypV",
        "outputId": "3ecf66c9-61de-40b2-fe3a-c02544734445"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SwinTransformer(\n",
            "  (stage1): StageModule(\n",
            "    (patch_partition): PatchMerging(\n",
            "      (patch_merge): Unfold(kernel_size=4, dilation=1, padding=0, stride=4)\n",
            "      (linear): Linear(in_features=48, out_features=96, bias=True)\n",
            "    )\n",
            "    (layers): ModuleList(\n",
            "      (0): ModuleList(\n",
            "        (0): SwinBlock(\n",
            "          (attention_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): WindowAttention(\n",
            "                (to_qkv): Linear(in_features=96, out_features=288, bias=False)\n",
            "                (to_out): Linear(in_features=96, out_features=96, bias=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (mlp_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): FeedForward(\n",
            "                (net): Sequential(\n",
            "                  (0): Linear(in_features=96, out_features=384, bias=True)\n",
            "                  (1): GELU(approximate=none)\n",
            "                  (2): Linear(in_features=384, out_features=96, bias=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (1): SwinBlock(\n",
            "          (attention_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): WindowAttention(\n",
            "                (cyclic_shift): CyclicShift()\n",
            "                (cyclic_back_shift): CyclicShift()\n",
            "                (to_qkv): Linear(in_features=96, out_features=288, bias=False)\n",
            "                (to_out): Linear(in_features=96, out_features=96, bias=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (mlp_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): FeedForward(\n",
            "                (net): Sequential(\n",
            "                  (0): Linear(in_features=96, out_features=384, bias=True)\n",
            "                  (1): GELU(approximate=none)\n",
            "                  (2): Linear(in_features=384, out_features=96, bias=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (stage2): StageModule(\n",
            "    (patch_partition): PatchMerging(\n",
            "      (patch_merge): Unfold(kernel_size=2, dilation=1, padding=0, stride=2)\n",
            "      (linear): Linear(in_features=384, out_features=192, bias=True)\n",
            "    )\n",
            "    (layers): ModuleList(\n",
            "      (0): ModuleList(\n",
            "        (0): SwinBlock(\n",
            "          (attention_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): WindowAttention(\n",
            "                (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n",
            "                (to_out): Linear(in_features=192, out_features=192, bias=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (mlp_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): FeedForward(\n",
            "                (net): Sequential(\n",
            "                  (0): Linear(in_features=192, out_features=768, bias=True)\n",
            "                  (1): GELU(approximate=none)\n",
            "                  (2): Linear(in_features=768, out_features=192, bias=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (1): SwinBlock(\n",
            "          (attention_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): WindowAttention(\n",
            "                (cyclic_shift): CyclicShift()\n",
            "                (cyclic_back_shift): CyclicShift()\n",
            "                (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n",
            "                (to_out): Linear(in_features=192, out_features=192, bias=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (mlp_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): FeedForward(\n",
            "                (net): Sequential(\n",
            "                  (0): Linear(in_features=192, out_features=768, bias=True)\n",
            "                  (1): GELU(approximate=none)\n",
            "                  (2): Linear(in_features=768, out_features=192, bias=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (stage3): StageModule(\n",
            "    (patch_partition): PatchMerging(\n",
            "      (patch_merge): Unfold(kernel_size=2, dilation=1, padding=0, stride=2)\n",
            "      (linear): Linear(in_features=768, out_features=384, bias=True)\n",
            "    )\n",
            "    (layers): ModuleList(\n",
            "      (0): ModuleList(\n",
            "        (0): SwinBlock(\n",
            "          (attention_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): WindowAttention(\n",
            "                (to_qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
            "                (to_out): Linear(in_features=384, out_features=384, bias=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (mlp_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): FeedForward(\n",
            "                (net): Sequential(\n",
            "                  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
            "                  (1): GELU(approximate=none)\n",
            "                  (2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (1): SwinBlock(\n",
            "          (attention_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): WindowAttention(\n",
            "                (cyclic_shift): CyclicShift()\n",
            "                (cyclic_back_shift): CyclicShift()\n",
            "                (to_qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
            "                (to_out): Linear(in_features=384, out_features=384, bias=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (mlp_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): FeedForward(\n",
            "                (net): Sequential(\n",
            "                  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
            "                  (1): GELU(approximate=none)\n",
            "                  (2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1): ModuleList(\n",
            "        (0): SwinBlock(\n",
            "          (attention_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): WindowAttention(\n",
            "                (to_qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
            "                (to_out): Linear(in_features=384, out_features=384, bias=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (mlp_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): FeedForward(\n",
            "                (net): Sequential(\n",
            "                  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
            "                  (1): GELU(approximate=none)\n",
            "                  (2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (1): SwinBlock(\n",
            "          (attention_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): WindowAttention(\n",
            "                (cyclic_shift): CyclicShift()\n",
            "                (cyclic_back_shift): CyclicShift()\n",
            "                (to_qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
            "                (to_out): Linear(in_features=384, out_features=384, bias=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (mlp_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): FeedForward(\n",
            "                (net): Sequential(\n",
            "                  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
            "                  (1): GELU(approximate=none)\n",
            "                  (2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (2): ModuleList(\n",
            "        (0): SwinBlock(\n",
            "          (attention_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): WindowAttention(\n",
            "                (to_qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
            "                (to_out): Linear(in_features=384, out_features=384, bias=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (mlp_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): FeedForward(\n",
            "                (net): Sequential(\n",
            "                  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
            "                  (1): GELU(approximate=none)\n",
            "                  (2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (1): SwinBlock(\n",
            "          (attention_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): WindowAttention(\n",
            "                (cyclic_shift): CyclicShift()\n",
            "                (cyclic_back_shift): CyclicShift()\n",
            "                (to_qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
            "                (to_out): Linear(in_features=384, out_features=384, bias=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (mlp_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): FeedForward(\n",
            "                (net): Sequential(\n",
            "                  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
            "                  (1): GELU(approximate=none)\n",
            "                  (2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (stage4): StageModule(\n",
            "    (patch_partition): PatchMerging(\n",
            "      (patch_merge): Unfold(kernel_size=2, dilation=1, padding=0, stride=2)\n",
            "      (linear): Linear(in_features=1536, out_features=768, bias=True)\n",
            "    )\n",
            "    (layers): ModuleList(\n",
            "      (0): ModuleList(\n",
            "        (0): SwinBlock(\n",
            "          (attention_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): WindowAttention(\n",
            "                (to_qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
            "                (to_out): Linear(in_features=768, out_features=768, bias=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (mlp_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): FeedForward(\n",
            "                (net): Sequential(\n",
            "                  (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                  (1): GELU(approximate=none)\n",
            "                  (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (1): SwinBlock(\n",
            "          (attention_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): WindowAttention(\n",
            "                (cyclic_shift): CyclicShift()\n",
            "                (cyclic_back_shift): CyclicShift()\n",
            "                (to_qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
            "                (to_out): Linear(in_features=768, out_features=768, bias=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (mlp_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): FeedForward(\n",
            "                (net): Sequential(\n",
            "                  (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                  (1): GELU(approximate=none)\n",
            "                  (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (mlp_head): Sequential(\n",
            "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    (1): Linear(in_features=768, out_features=1000, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss 함수와 Optimizer 설정\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "6DXFHZT_jKI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training을 위한 함수\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # 예측 오류 계산\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # 역전파\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ],
      "metadata": {
        "id": "Vm4NOTp5jSd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test를 위한 함수\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "b6gYD-vwjTmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 963
        },
        "id": "-92IJfL2jVT9",
        "outputId": "870752e9-7c93-4d1a-9cfb-3fb5b8c52727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 6.857241  [    0/50000]\n",
            "loss: 2.275917  [ 6400/50000]\n",
            "loss: 2.103641  [12800/50000]\n",
            "loss: 2.075073  [19200/50000]\n",
            "loss: 2.197557  [25600/50000]\n",
            "loss: 1.989382  [32000/50000]\n",
            "loss: 2.037771  [38400/50000]\n",
            "loss: 1.864286  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 28.7%, Avg loss: 1.901600 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.938413  [    0/50000]\n",
            "loss: 1.870061  [ 6400/50000]\n",
            "loss: 1.870908  [12800/50000]\n",
            "loss: 1.906039  [19200/50000]\n",
            "loss: 1.944371  [25600/50000]\n",
            "loss: 1.882620  [32000/50000]\n",
            "loss: 2.000605  [38400/50000]\n",
            "loss: 1.946506  [44800/50000]\n",
            "Test Error: \n",
            " Accuracy: 15.3%, Avg loss: 2.258248 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.312144  [    0/50000]\n",
            "loss: 2.007992  [ 6400/50000]\n",
            "loss: 1.907961  [12800/50000]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-f60795a63e70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-73-707066c26b48>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# 역전파\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mIf\u001b[0m \u001b[0myou\u001b[0m \u001b[0mrun\u001b[0m \u001b[0many\u001b[0m \u001b[0mforward\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mgrad_outputs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;32mor\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mspecified\u001b[0m \u001b[0mCUDA\u001b[0m \u001b[0mstream\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msee\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0;34m:\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mStream\u001b[0m \u001b[0msemantics\u001b[0m \u001b[0mof\u001b[0m \u001b[0mbackward\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mbwd\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msemantics\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0mArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tsm14GRSj1Ih"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1xih9AQpJEmMvLxcQGnZv_9B4ml4m42nP",
      "authorship_tag": "ABX9TyN06B8qNtEWUCay9JNn8ccl",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bd5b0a3e5d82409eb2c7686c8f36f04f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a6ceab93c0b146339ea81cffa959169d",
              "IPY_MODEL_a8a78456c5db49b5b696113b8af721b1",
              "IPY_MODEL_cc217d2367af42ff811dc7a5afa35080"
            ],
            "layout": "IPY_MODEL_5658b3a0bb2849de99f59840f92fcda2"
          }
        },
        "a6ceab93c0b146339ea81cffa959169d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f189964d47714bfb9f0f7b1c0e253b04",
            "placeholder": "​",
            "style": "IPY_MODEL_0a5ae50dcf63492a9a447cac3cbe479c",
            "value": "100%"
          }
        },
        "a8a78456c5db49b5b696113b8af721b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dced840ebad74ceea562c848b8f25fff",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2d2b979314d4470592684f290d132e28",
            "value": 170498071
          }
        },
        "cc217d2367af42ff811dc7a5afa35080": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af3dd2a7719d4f45bfd6804e35da8c60",
            "placeholder": "​",
            "style": "IPY_MODEL_8640ff7775134332ae431018b37a8ba7",
            "value": " 170498071/170498071 [00:01&lt;00:00, 109551589.82it/s]"
          }
        },
        "5658b3a0bb2849de99f59840f92fcda2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f189964d47714bfb9f0f7b1c0e253b04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a5ae50dcf63492a9a447cac3cbe479c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dced840ebad74ceea562c848b8f25fff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d2b979314d4470592684f290d132e28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "af3dd2a7719d4f45bfd6804e35da8c60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8640ff7775134332ae431018b37a8ba7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
